{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec947103-90b2-4e96-8e48-dbbddcfaf269",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 01:27:44.515105: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-20 01:27:44.546448: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-20 01:27:44.546484: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-20 01:27:44.547492: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-20 01:27:44.552963: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-20 01:27:45.383317: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead, AutoModelForCausalLM\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel, GPT2Config, GPT2ForQuestionAnswering\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset\n",
    "import copy\n",
    "import pandas as pd\n",
    "import re\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "# Medium has 24 layers/GPT2Blocks\n",
    "med_tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "# Large has 36 layers/GPT2Blocks\n",
    "large_tokenizer = AutoTokenizer.from_pretrained(\"gpt2-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f59482c-f128-4a97-9648-9f74b2e77ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SQuAD dataset, roughly 9:1:1, train:valid:test\n",
    "train_squad = load_dataset(\"squad\")[\"train\"].train_test_split(test_size=0.12)\n",
    "train_dataset = train_squad[\"train\"]\n",
    "valid_dataset = load_dataset(\"squad\")[\"validation\"]\n",
    "test_dataset = train_squad[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70238b17-5141-4bba-8f28-4f5e01647c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '570d61a5b3d812140066d7a7', 'title': 'Valencia', 'context': 'In March 2012, the newspaper El Mundo published a story according to which FGV had instructed employees who were to testify at the crash commission investigation, providing a set of possible questions and guidelines to prepare the answers. In April 2013, the television program Salvados questioned the official version of the incident as there were indications that the Valencian Government had tried to downplay the accident, which coincided with the visit of the pope to Valencia, or even to hide evidence, as the book of train breakdowns was never found. The day after the broadcast of this report, which received extensive media coverage, several voices called for the reopening of the investigation. The investigation was effectively reopened and the accident is currently under re-examination.', 'question': 'What evidence related to the crash remains missing?', 'answers': {'text': ['book of train breakdowns'], 'answer_start': [516]}}\n"
     ]
    }
   ],
   "source": [
    "# sanity checking\n",
    "len(train_dataset), len(valid_dataset), len(test_dataset)\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15bf3a1f-7430-4dfc-b224-3cb0f6573887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(examples, tokenizer):\n",
    "    contexes = examples[\"context\"]\n",
    "    questions = examples[\"question\"]\n",
    "    answers = examples[\"answers\"]\n",
    "    samples = [f\"{context}\\n{question}\\n{answer['text'][0]}\" for context, question, answer in zip(contexes, questions, answers)]\n",
    "    return tokenizer(samples, truncation=True, padding=\"max_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94a80f36-9ff3-4859-81df-6bb678b94b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc7f1737a04842ebb71630938482d1f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/77087 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f1b92185c1e420f97ea8a4e35d12194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "med_tokenizer.padding_side = \"left\"\n",
    "if med_tokenizer.pad_token is None:\n",
    "    med_tokenizer.pad_token = med_tokenizer.eos_token\n",
    "\n",
    "train_dataset_med = train_dataset.map(lambda x: encode(x, med_tokenizer), batched=True)\n",
    "valid_dataset_med = valid_dataset.map(lambda x: encode(x, med_tokenizer), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44c00b18-ec60-4446-be94-06fde458436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_decodes(decodes):\n",
    "    for i, d in enumerate(decodes):\n",
    "        print(f\"{i}: {d}\\n\")\n",
    "\n",
    "def get_question(sample):\n",
    "    return f'{sample[\"context\"]}\\n{sample[\"question\"]}'\n",
    "\n",
    "def get_prediction(prompt, model, tokenizer, max_tokens=50):\n",
    "    model.eval()\n",
    "    input_text = [prompt]\n",
    "    prompts = [torch.tensor(tokenizer.encode(s)).unsqueeze(0) for s in input_text]\n",
    "    out0 = [tokenizer.decode(\n",
    "        model.generate(p, \n",
    "                            max_length=p.shape[-1]+max_tokens)[0,:]) for p in prompts]\n",
    "    print_decodes(out0)\n",
    "    return out0\n",
    "\n",
    "def get_model_answer(index, dataset, model, tokenizer, max_tokens=50):\n",
    "    prompt = get_question(dataset[index])\n",
    "    prediction = get_prediction(prompt, model, tokenizer, max_tokens)\n",
    "    print(\"\\nAnswer key: \", dataset[index][\"answers\"][\"text\"][0])\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ddf6db8-0181-492e-9d19-0406901fa69f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ce776d5282456baf4dbec47859723c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.37G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c68a9c4fec846db81ea168513a3aa64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/144 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# download pruned model\n",
    "gpt2_med_s23 = AutoModelForCausalLM.from_pretrained(\"han2lin/gpt2_med_s23\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3048c5db-335b-4621-96d9-9d1616a6aa6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num layers=23\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "print(f\"num layers={gpt2_med_s23.config.n_layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52b9cb5d-709a-43ef-9c5c-be8a0ff91811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Hunting and gathering was humanity's first and most successful adaptation, occupying at least 90 percent of human history. Following the invention of agriculture, hunter-gatherers have been displaced or conquered by farming or pastoralist groups in most parts of the world.\n",
      "What are the basic types of agricultural groups?\n",
      "The most common agricultural groups are hunter-gatherers, who live in groups of several hundred individuals, and pastoralists, who live in groups of several hundred individuals. The most common types of agricultural groups are:\n",
      "1. Group A:\n",
      "\n",
      "\n",
      "Answer key:  farming or pastoralist groups\n"
     ]
    }
   ],
   "source": [
    "# Test base model prediction\n",
    "INDEX = 1\n",
    "answer = get_model_answer(INDEX, train_dataset_med, gpt2_med_s23, med_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7629cafb-e296-446c-9fbf-91bcd348468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for setting up wandb:\n",
    "# import wandb\n",
    "# wandb.login()\n",
    "\n",
    "# wandb.init(\n",
    "#     project=\"gpt2-pruning\",\n",
    "#     config={\n",
    "#         # \"batch_size\": BATCH_SIZE,\n",
    "#         # \"learning_rate\": LEARNING_RATE,\n",
    "#         \"dataset\": \"SQuAD\",\n",
    "#     },\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4949f3c2-f1be-41d9-a56b-cfd7a5569ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-5\n",
    "LOGGING_STEPS = 1000\n",
    "SAVE_STEPS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03367590-4a09-4612-ac35-ec9486837f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_gpt2(model, \n",
    "                   tokenizer, \n",
    "                   train_dataset, \n",
    "                   valid_dataset, \n",
    "                   train_output_dir,\n",
    "                   save_model_dir):\n",
    "    # Create data collator for language modeling\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    # Set training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir = train_output_dir, \n",
    "        evaluation_strategy = \"steps\", \n",
    "        disable_tqdm = False,\n",
    "        logging_steps = LOGGING_STEPS,\n",
    "        logging_strategy = \"steps\",\n",
    "        save_steps = SAVE_STEPS,\n",
    "        num_train_epochs = EPOCHS,\n",
    "        per_device_train_batch_size = BATCH_SIZE,\n",
    "        per_device_eval_batch_size = BATCH_SIZE,\n",
    "        learning_rate = LEARNING_RATE,\n",
    "        # optim=\"paged_adamw_32bit\",\n",
    "        report_to = \"wandb\",\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    log_history = pd.DataFrame(trainer.state.log_history)\n",
    "    print(log_history)\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    model.save_pretrained(save_model_dir)\n",
    "    tokenizer.save_pretrained(save_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c005e4-b03b-4a45-9b37-e0beaeaf3865",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_gpt2(base_model, \n",
    "               base_tokenizer, \n",
    "               small_train_dataset['input_ids'], \n",
    "               small_valid_dataset['input_ids'],\n",
    "               \"train_log\",\n",
    "               \"trained_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
