{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ee1fdd0-0928-4076-b736-382d07f97a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c71406ab64574fe28314906bd3d99857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a39bef95d44d70ac7677f45da8e7dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c1046e8029b4ae58a2351620b7aa532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "284045280ace44259b6731bf7a971747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dac275a81ab4d51bba9820849d06d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cafbe7962a3d4c0da377fc83ace135a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "779bc01fd96743969e2a2ec6619dc24a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead, AutoModelForCausalLM\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel, GPT2Config, GPT2ForQuestionAnswering\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset\n",
    "import copy\n",
    "import pandas as pd\n",
    "import re\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "# Medium has 24 layers/GPT2Blocks\n",
    "med_tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
    "med_model = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\", pad_token_id = med_tokenizer.eos_token_id)\n",
    "\n",
    "# Large has 36 layers/GPT2Blocks\n",
    "large_tokenizer = AutoTokenizer.from_pretrained(\"gpt2-large\")\n",
    "large_model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\", pad_token_id = large_tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43546c07-99d4-43b6-8326-81f4cc5b8415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pruned(model: GPT2Model, layers: list):\n",
    "    if model.config.n_layer < len(layers):\n",
    "        print(f\"List of layers too long\")\n",
    "        return\n",
    "    if any([l for l in layers if l >= model.config.n_layer or l < 0]):\n",
    "        print(f\"All layers specified must be indexes _less_ than number of layers available\")\n",
    "        return\n",
    "    \n",
    "    layers.sort()\n",
    "    print(f\"Pruning {len(layers)} layer(s)...\")\n",
    "    pruned_config = copy.deepcopy(model.config)\n",
    "    pruned_config.n_layer -= len(layers)\n",
    "    pruned_model = GPT2LMHeadModel(pruned_config)\n",
    "\n",
    "    pruned_states = []\n",
    "    for layer in layers:\n",
    "        pruned_states += list(filter(\n",
    "            lambda s: re.search(f'transformer.h\\.{layer}\\.',s) is not None,\n",
    "            model.state_dict().keys()))\n",
    "    print(f\"Dropping these states: {pruned_states[:3]}+...\")\n",
    "\n",
    "    base = dict(model.named_parameters())\n",
    "    pruned = dict(pruned_model.named_parameters())\n",
    "\n",
    "    prev_base_idx = -1\n",
    "    pruned_idx = 0\n",
    "    prev_skipped = False\n",
    "    copied_states = []\n",
    "    \n",
    "    for k, v in model.named_parameters():\n",
    "        base_idx = re.search(r\".h.([0-9]+).\", k)\n",
    "        if base_idx:\n",
    "            base_idx = int(base_idx.group(1))\n",
    "            if base_idx in layers:\n",
    "                # the next base layer to copy should go into the current pruned layer\n",
    "                if prev_base_idx != base_idx and not prev_skipped and pruned_idx > 0:\n",
    "                    pruned_idx += 1\n",
    "                prev_skipped = True\n",
    "                continue                \n",
    "            if prev_base_idx != base_idx and not prev_skipped and base_idx > 0:\n",
    "                pruned_idx += 1\n",
    "            prev_skipped = False\n",
    "            copied_states.append(k)\n",
    "            k = re.sub(f\".h.{base_idx}.\", f\".h.{pruned_idx}.\", k)\n",
    "            pruned[k].data = copy.deepcopy(v.data)\n",
    "            prev_base_idx = base_idx\n",
    "        else:\n",
    "            copied_states.append(k)\n",
    "            pruned[k].data = copy.deepcopy(v.data)\n",
    "            \n",
    "    print(f\"Copied these states into the pruned model: {copied_states[:3]}+...\")\n",
    "    # print(f\"Pruned model architecture: {pruned_model}\")\n",
    "    return pruned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f91bed02-a52e-4eec-afb0-b52ff6ba1d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning 1 layer(s)...\n",
      "Dropping these states: ['transformer.h.23.ln_1.weight', 'transformer.h.23.ln_1.bias', 'transformer.h.23.attn.c_attn.weight']+...\n",
      "Copied these states into the pruned model: ['transformer.wte.weight', 'transformer.wpe.weight', 'transformer.h.0.ln_1.weight']+...\n",
      "Pruning 1 layer(s)...\n",
      "Dropping these states: ['transformer.h.22.ln_1.weight', 'transformer.h.22.ln_1.bias', 'transformer.h.22.attn.c_attn.weight']+...\n",
      "Copied these states into the pruned model: ['transformer.wte.weight', 'transformer.wpe.weight', 'transformer.h.0.ln_1.weight']+...\n",
      "Pruning 2 layer(s)...\n",
      "Dropping these states: ['transformer.h.21.ln_1.weight', 'transformer.h.21.ln_1.bias', 'transformer.h.21.attn.c_attn.weight']+...\n",
      "Copied these states into the pruned model: ['transformer.wte.weight', 'transformer.wpe.weight', 'transformer.h.0.ln_1.weight']+...\n",
      "Pruning 4 layer(s)...\n",
      "Dropping these states: ['transformer.h.19.ln_1.weight', 'transformer.h.19.ln_1.bias', 'transformer.h.19.attn.c_attn.weight']+...\n",
      "Copied these states into the pruned model: ['transformer.wte.weight', 'transformer.wpe.weight', 'transformer.h.0.ln_1.weight']+...\n",
      "Pruning 8 layer(s)...\n",
      "Dropping these states: ['transformer.h.15.ln_1.weight', 'transformer.h.15.ln_1.bias', 'transformer.h.15.attn.c_attn.weight']+...\n",
      "Copied these states into the pruned model: ['transformer.wte.weight', 'transformer.wpe.weight', 'transformer.h.0.ln_1.weight']+...\n",
      "Pruning 1 layer(s)...\n",
      "Dropping these states: ['transformer.h.35.ln_1.weight', 'transformer.h.35.ln_1.bias', 'transformer.h.35.attn.c_attn.weight']+...\n",
      "Copied these states into the pruned model: ['transformer.wte.weight', 'transformer.wpe.weight', 'transformer.h.0.ln_1.weight']+...\n",
      "Pruning 1 layer(s)...\n",
      "Dropping these states: ['transformer.h.34.ln_1.weight', 'transformer.h.34.ln_1.bias', 'transformer.h.34.attn.c_attn.weight']+...\n",
      "Copied these states into the pruned model: ['transformer.wte.weight', 'transformer.wpe.weight', 'transformer.h.0.ln_1.weight']+...\n",
      "Pruning 2 layer(s)...\n",
      "Dropping these states: ['transformer.h.33.ln_1.weight', 'transformer.h.33.ln_1.bias', 'transformer.h.33.attn.c_attn.weight']+...\n",
      "Copied these states into the pruned model: ['transformer.wte.weight', 'transformer.wpe.weight', 'transformer.h.0.ln_1.weight']+...\n",
      "Pruning 4 layer(s)...\n",
      "Dropping these states: ['transformer.h.31.ln_1.weight', 'transformer.h.31.ln_1.bias', 'transformer.h.31.attn.c_attn.weight']+...\n",
      "Copied these states into the pruned model: ['transformer.wte.weight', 'transformer.wpe.weight', 'transformer.h.0.ln_1.weight']+...\n",
      "Pruning 8 layer(s)...\n",
      "Dropping these states: ['transformer.h.27.ln_1.weight', 'transformer.h.27.ln_1.bias', 'transformer.h.27.attn.c_attn.weight']+...\n",
      "Copied these states into the pruned model: ['transformer.wte.weight', 'transformer.wpe.weight', 'transformer.h.0.ln_1.weight']+...\n",
      "Pruning 16 layer(s)...\n",
      "Dropping these states: ['transformer.h.19.ln_1.weight', 'transformer.h.19.ln_1.bias', 'transformer.h.19.attn.c_attn.weight']+...\n",
      "Copied these states into the pruned model: ['transformer.wte.weight', 'transformer.wpe.weight', 'transformer.h.0.ln_1.weight']+...\n"
     ]
    }
   ],
   "source": [
    "pruned_med_s23 = make_pruned(med_model, [23])\n",
    "pruned_med_s22  = make_pruned(med_model, [22])\n",
    "pruned_med_s21e22  = make_pruned(med_model, [21, 22])\n",
    "pruned_med_s19e22  = make_pruned(med_model, [19, 20, 21, 22])\n",
    "pruned_med_s15e22  = make_pruned(med_model, [15, 16, 17, 18, 19, 20, 21, 22])\n",
    "\n",
    "pruned_large_s35 = make_pruned(large_model, [35])\n",
    "pruned_large_s34  = make_pruned(large_model, [34])\n",
    "pruned_large_s33e34  = make_pruned(large_model, [33, 34])\n",
    "pruned_large_s31e34  = make_pruned(large_model, [31, 32, 33, 34])\n",
    "pruned_large_s27e34  = make_pruned(large_model, [27, 28, 29, 30, 31, 32, 33, 34])\n",
    "pruned_large_s19e34  = make_pruned(large_model, [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "705f6e1d-47d4-46b6-b95b-ce3fb50406bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [pruned_med_s23,\n",
    "          pruned_med_s22,\n",
    "          pruned_med_s21e22,\n",
    "          pruned_med_s19e22,\n",
    "          pruned_med_s15e22,\n",
    "          pruned_large_s35,\n",
    "          pruned_large_s34,\n",
    "          pruned_large_s33e34,\n",
    "          pruned_large_s31e34,\n",
    "          pruned_large_s27e34,\n",
    "          pruned_large_s19e34]\n",
    "names=[\"gpt2_med_s23\",\n",
    "       \"gpt2_med_s22\",\n",
    "       \"gpt2_med_s21e22\", \n",
    "       \"gpt2_med_s19e22\", \n",
    "       \"gpt2_med_s15e22\", \n",
    "       \"gpt2_large_s35\", \n",
    "       \"gpt2_large_s34\", \n",
    "       \"gpt2_large_s33e34\", \n",
    "       \"gpt2_large_s31e34\", \n",
    "       \"gpt2_large_s27e34\",\n",
    "       \"gpt2_large_s19e34\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25f2991-d83a-4df6-bc72-77447ccfd399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16a959c2-9c79-4173-9b9d-81fee6255d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee02bf1919004ad1b04d922512c2f3b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1d574aac06e4cf8bcd7c90c5f74b45c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.37G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31162d9a21874289bf3e99ef259ab8ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.32G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e3ded33581402aa174f4f4daf4ab46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8933dfbd1b9940b58996fa35dfa96c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.02G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648ee1f36be348c59a50549fe2f9eeea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.02G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd484dc72a224f30b9962c74d78d1cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.02G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b46ba269821422dacfb8d39f1403308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e3092dd36645478593d58efc9e813b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "426898c179104fce8c4f47d0c8e9cc0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c7199f01a7143d68f4b90154f33690c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.84G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for model, name in zip(models, names):\n",
    "    model.push_to_hub(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
