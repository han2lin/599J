{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b9b95f-918b-4ef6-afa9-01919868cdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loosely based on https://www.linkedin.com/pulse/fine-tuning-gpt-2-large-language-model-unlocking-its-adamson-mbcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "459935e1-bade-46a7-b21d-f9f749a08da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 22:42:19.104128: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-19 22:42:19.134350: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-19 22:42:19.134390: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-19 22:42:19.135347: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-19 22:42:19.140836: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-19 22:42:19.850556: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead, AutoModelForCausalLM\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel, GPT2Config, GPT2ForQuestionAnswering\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset\n",
    "import copy\n",
    "import pandas as pd\n",
    "import re\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\", pad_token_id = base_tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36dfa22f-a413-4146-81be-f38c39503275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model architecture:\n",
      " GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 1024)\n",
      "    (wpe): Embedding(1024, 1024)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-23): 24 x GPT2Block(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(f\"Base model architecture:\\n\", base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1738e290-92f9-44b5-bb78-386351dfdd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SQuAD dataset, roughly 9:1:1, train:valid:test\n",
    "train_squad = load_dataset(\"squad\")[\"train\"].train_test_split(test_size=0.12)\n",
    "train_dataset = train_squad[\"train\"]\n",
    "valid_dataset = load_dataset(\"squad\")[\"validation\"]\n",
    "test_dataset = train_squad[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b66d1e7-b08b-4b7e-a855-8a452e1e770e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '56d3e4762ccc5a1400d82f21', 'title': 'To_Kill_a_Mockingbird', 'context': 'During the years immediately following the novel\\'s publication, Harper Lee enjoyed the attention its popularity garnered her, granting interviews, visiting schools, and attending events honoring the book. In 1961, when To Kill a Mockingbird was in its 41st week on the bestseller list, it was awarded the Pulitzer Prize, stunning Lee. It also won the Brotherhood Award of the National Conference of Christians and Jews in the same year, and the Paperback of the Year award from Bestsellers magazine in 1962. Starting in 1964, Lee began to turn down interviews, complaining that the questions were monotonous, and grew concerned that attention she received bordered on the kind of publicity celebrities sought. Since the, she declined talking with reporters about the book. She also steadfastly refused to provide an introduction, writing in 1995: \"Introductions inhibit pleasure, they kill the joy of anticipation, they frustrate curiosity. The only good thing about Introductions is that in some cases they delay the dose to come. Mockingbird still says what it has to say; it has managed to survive the years without preamble.\"', 'question': 'What major award did the book receive in 1961?', 'answers': {'text': ['the Pulitzer Prize'], 'answer_start': [301]}}\n"
     ]
    }
   ],
   "source": [
    "# sanity checking\n",
    "len(train_dataset), len(valid_dataset), len(test_dataset)\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4b31680-3b13-4b08-8c5e-ced7a0d9daca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee3cf0b546f4874aa77c0b7f6e653cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/77087 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_tokenizer.padding_side = \"left\"\n",
    "if base_tokenizer.pad_token is None:\n",
    "    base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "\n",
    "def encode(examples):\n",
    "    contexes = examples[\"context\"]\n",
    "    questions = examples[\"question\"]\n",
    "    answers = examples[\"answers\"]\n",
    "    samples = [f\"{context}\\n{question}\\n{answer['text'][0]}\" for context, question, answer in zip(contexes, questions, answers)]\n",
    "    return base_tokenizer(samples, truncation=True, padding=\"max_length\")\n",
    "\n",
    "train_dataset = train_dataset.map(encode, batched=True)\n",
    "valid_dataset = valid_dataset.map(encode, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34506cdf-76da-41e1-9430-5aacac208517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get smaller datasets to test with\n",
    "small_train_dataset = train_dataset.shuffle(seed=42).select(range(100))\n",
    "small_valid_dataset = valid_dataset.shuffle(seed=42).select(range(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28529ca5-e662-4978-a900-0c750c4e9204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_decodes(decodes):\n",
    "    for i, d in enumerate(decodes):\n",
    "        print(f\"{i}: {d}\\n\")\n",
    "\n",
    "def get_question(sample):\n",
    "    return f'{sample[\"context\"]}\\n{sample[\"question\"]}'\n",
    "\n",
    "def get_prediction(prompt, model, tokenizer, max_tokens=50):\n",
    "    model.eval()\n",
    "    input_text = [prompt]\n",
    "    prompts = [torch.tensor(tokenizer.encode(s)).unsqueeze(0) for s in input_text]\n",
    "    out0 = [tokenizer.decode(\n",
    "        model.generate(p, \n",
    "                            max_length=p.shape[-1]+max_tokens)[0,:]) for p in prompts]\n",
    "    print_decodes(out0)\n",
    "    return out0\n",
    "\n",
    "def get_model_answer(index, dataset, model, tokenizer, max_tokens=50):\n",
    "    prompt = get_question(dataset[index])\n",
    "    prediction = get_prediction(prompt, model, tokenizer, max_tokens)\n",
    "    print(\"\\nAnswer key: \", dataset[index][\"answers\"][\"text\"][0])\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "752e63ff-e6af-49d8-9afb-7369ca422353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Federal law originates with the Constitution, which gives Congress the power to enact statutes for certain limited purposes like regulating interstate commerce. The United States Code is the official compilation and codification of the general and permanent federal statutes. Many statutes give executive branch agencies the power to create regulations, which are published in the Federal Register and codified into the Code of Federal Regulations. Regulations generally also carry the force of law under the Chevron doctrine. Many lawsuits turn on the meaning of a federal statute or regulation, and judicial interpretations of such meaning carry legal force under the principle of stare decisis.\n",
      "Who do the statutes give the power of creating regulations?\n",
      "\n",
      "The statutes give the power to create regulations to the executive branch agencies, which are the federal agencies that are responsible for the administration of the federal government. The statutes also give the power to create regulations to the states, which are the states that\n",
      "\n",
      "\n",
      "Answer key:  executive branch agencies\n"
     ]
    }
   ],
   "source": [
    "# Test base model prediction\n",
    "INDEX = 1\n",
    "answer = get_model_answer(INDEX, small_train_dataset, base_model, base_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f4ffa82-d132-439d-a693-00a8e1b71b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pruned(model: GPT2Model, layers: list):\n",
    "    if model.config.n_layer < len(layers):\n",
    "        print(f\"List of layers too long\")\n",
    "        return\n",
    "    if any([l for l in layers if l >= model.config.n_layer or l < 0]):\n",
    "        print(f\"All layers specified must be indexes _less_ than number of layers available\")\n",
    "        return\n",
    "    \n",
    "    layers.sort()\n",
    "    print(f\"Pruning {len(layers)} layer(s)...\")\n",
    "    pruned_config = copy.deepcopy(model.config)\n",
    "    pruned_config.n_layer -= len(layers)\n",
    "    pruned_model = GPT2LMHeadModel(pruned_config)\n",
    "\n",
    "    pruned_states = []\n",
    "    for layer in layers:\n",
    "        pruned_states += list(filter(\n",
    "            lambda s: re.search(f'transformer.h\\.{layer}\\.',s) is not None,\n",
    "            model.state_dict().keys()))\n",
    "    print(f\"Dropping these states: {pruned_states}\")\n",
    "\n",
    "    base = dict(model.named_parameters())\n",
    "    pruned = dict(pruned_model.named_parameters())\n",
    "\n",
    "    prev_base_idx = -1\n",
    "    pruned_idx = 0\n",
    "    prev_skipped = False\n",
    "    copied_states = []\n",
    "    \n",
    "    for k, v in model.named_parameters():\n",
    "        base_idx = re.search(r\".h.([0-9]+).\", k)\n",
    "        if base_idx:\n",
    "            base_idx = int(base_idx.group(1))\n",
    "            if base_idx in layers:\n",
    "                # the next base layer to copy should go into the current pruned layer\n",
    "                if prev_base_idx != base_idx and not prev_skipped and pruned_idx > 0:\n",
    "                    pruned_idx += 1\n",
    "                prev_skipped = True\n",
    "                continue                \n",
    "            if prev_base_idx != base_idx and not prev_skipped and base_idx > 0:\n",
    "                pruned_idx += 1\n",
    "            prev_skipped = False\n",
    "            copied_states.append(k)\n",
    "            k = re.sub(f\".h.{base_idx}.\", f\".h.{pruned_idx}.\", k)\n",
    "            pruned[k].data = copy.deepcopy(v.data)\n",
    "            prev_base_idx = base_idx\n",
    "        else:\n",
    "            copied_states.append(k)\n",
    "            pruned[k].data = copy.deepcopy(v.data)\n",
    "            \n",
    "    print(f\"Copied these states into the pruned model: {copied_states}\")\n",
    "    print(f\"Pruned model architecture: {pruned_model}\")\n",
    "    return pruned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb33d8b0-99ae-47d0-aa84-13f91664043a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning 1 layer(s)...\n",
      "Dropping these states: ['transformer.h.23.ln_1.weight', 'transformer.h.23.ln_1.bias', 'transformer.h.23.attn.c_attn.weight', 'transformer.h.23.attn.c_attn.bias', 'transformer.h.23.attn.c_proj.weight', 'transformer.h.23.attn.c_proj.bias', 'transformer.h.23.ln_2.weight', 'transformer.h.23.ln_2.bias', 'transformer.h.23.mlp.c_fc.weight', 'transformer.h.23.mlp.c_fc.bias', 'transformer.h.23.mlp.c_proj.weight', 'transformer.h.23.mlp.c_proj.bias']\n",
      "Copied these states into the pruned model: ['transformer.wte.weight', 'transformer.wpe.weight', 'transformer.h.0.ln_1.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_attn.bias', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.0.ln_2.weight', 'transformer.h.0.ln_2.bias', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.1.ln_1.weight', 'transformer.h.1.ln_1.bias', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.1.attn.c_attn.bias', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.1.ln_2.weight', 'transformer.h.1.ln_2.bias', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.1.mlp.c_fc.bias', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.2.ln_1.weight', 'transformer.h.2.ln_1.bias', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.2.attn.c_attn.bias', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.2.ln_2.weight', 'transformer.h.2.ln_2.bias', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.2.mlp.c_fc.bias', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.3.ln_1.weight', 'transformer.h.3.ln_1.bias', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.3.attn.c_attn.bias', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.3.ln_2.weight', 'transformer.h.3.ln_2.bias', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.3.mlp.c_fc.bias', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.4.ln_1.weight', 'transformer.h.4.ln_1.bias', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.4.attn.c_attn.bias', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.4.ln_2.weight', 'transformer.h.4.ln_2.bias', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.4.mlp.c_fc.bias', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.5.ln_1.weight', 'transformer.h.5.ln_1.bias', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.5.attn.c_attn.bias', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.5.ln_2.weight', 'transformer.h.5.ln_2.bias', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.5.mlp.c_fc.bias', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.6.ln_1.weight', 'transformer.h.6.ln_1.bias', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.6.attn.c_attn.bias', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.6.ln_2.weight', 'transformer.h.6.ln_2.bias', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.6.mlp.c_fc.bias', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.7.ln_1.weight', 'transformer.h.7.ln_1.bias', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.7.attn.c_attn.bias', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.7.ln_2.weight', 'transformer.h.7.ln_2.bias', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.7.mlp.c_fc.bias', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.8.ln_1.weight', 'transformer.h.8.ln_1.bias', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.8.attn.c_attn.bias', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.8.ln_2.weight', 'transformer.h.8.ln_2.bias', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.8.mlp.c_fc.bias', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.9.ln_1.weight', 'transformer.h.9.ln_1.bias', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.9.attn.c_attn.bias', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.9.ln_2.weight', 'transformer.h.9.ln_2.bias', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.9.mlp.c_fc.bias', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.10.ln_1.weight', 'transformer.h.10.ln_1.bias', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.10.attn.c_attn.bias', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.10.ln_2.weight', 'transformer.h.10.ln_2.bias', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.10.mlp.c_fc.bias', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.11.ln_1.weight', 'transformer.h.11.ln_1.bias', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.11.attn.c_attn.bias', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.11.ln_2.weight', 'transformer.h.11.ln_2.bias', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.11.mlp.c_fc.bias', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.11.mlp.c_proj.bias', 'transformer.h.12.ln_1.weight', 'transformer.h.12.ln_1.bias', 'transformer.h.12.attn.c_attn.weight', 'transformer.h.12.attn.c_attn.bias', 'transformer.h.12.attn.c_proj.weight', 'transformer.h.12.attn.c_proj.bias', 'transformer.h.12.ln_2.weight', 'transformer.h.12.ln_2.bias', 'transformer.h.12.mlp.c_fc.weight', 'transformer.h.12.mlp.c_fc.bias', 'transformer.h.12.mlp.c_proj.weight', 'transformer.h.12.mlp.c_proj.bias', 'transformer.h.13.ln_1.weight', 'transformer.h.13.ln_1.bias', 'transformer.h.13.attn.c_attn.weight', 'transformer.h.13.attn.c_attn.bias', 'transformer.h.13.attn.c_proj.weight', 'transformer.h.13.attn.c_proj.bias', 'transformer.h.13.ln_2.weight', 'transformer.h.13.ln_2.bias', 'transformer.h.13.mlp.c_fc.weight', 'transformer.h.13.mlp.c_fc.bias', 'transformer.h.13.mlp.c_proj.weight', 'transformer.h.13.mlp.c_proj.bias', 'transformer.h.14.ln_1.weight', 'transformer.h.14.ln_1.bias', 'transformer.h.14.attn.c_attn.weight', 'transformer.h.14.attn.c_attn.bias', 'transformer.h.14.attn.c_proj.weight', 'transformer.h.14.attn.c_proj.bias', 'transformer.h.14.ln_2.weight', 'transformer.h.14.ln_2.bias', 'transformer.h.14.mlp.c_fc.weight', 'transformer.h.14.mlp.c_fc.bias', 'transformer.h.14.mlp.c_proj.weight', 'transformer.h.14.mlp.c_proj.bias', 'transformer.h.15.ln_1.weight', 'transformer.h.15.ln_1.bias', 'transformer.h.15.attn.c_attn.weight', 'transformer.h.15.attn.c_attn.bias', 'transformer.h.15.attn.c_proj.weight', 'transformer.h.15.attn.c_proj.bias', 'transformer.h.15.ln_2.weight', 'transformer.h.15.ln_2.bias', 'transformer.h.15.mlp.c_fc.weight', 'transformer.h.15.mlp.c_fc.bias', 'transformer.h.15.mlp.c_proj.weight', 'transformer.h.15.mlp.c_proj.bias', 'transformer.h.16.ln_1.weight', 'transformer.h.16.ln_1.bias', 'transformer.h.16.attn.c_attn.weight', 'transformer.h.16.attn.c_attn.bias', 'transformer.h.16.attn.c_proj.weight', 'transformer.h.16.attn.c_proj.bias', 'transformer.h.16.ln_2.weight', 'transformer.h.16.ln_2.bias', 'transformer.h.16.mlp.c_fc.weight', 'transformer.h.16.mlp.c_fc.bias', 'transformer.h.16.mlp.c_proj.weight', 'transformer.h.16.mlp.c_proj.bias', 'transformer.h.17.ln_1.weight', 'transformer.h.17.ln_1.bias', 'transformer.h.17.attn.c_attn.weight', 'transformer.h.17.attn.c_attn.bias', 'transformer.h.17.attn.c_proj.weight', 'transformer.h.17.attn.c_proj.bias', 'transformer.h.17.ln_2.weight', 'transformer.h.17.ln_2.bias', 'transformer.h.17.mlp.c_fc.weight', 'transformer.h.17.mlp.c_fc.bias', 'transformer.h.17.mlp.c_proj.weight', 'transformer.h.17.mlp.c_proj.bias', 'transformer.h.18.ln_1.weight', 'transformer.h.18.ln_1.bias', 'transformer.h.18.attn.c_attn.weight', 'transformer.h.18.attn.c_attn.bias', 'transformer.h.18.attn.c_proj.weight', 'transformer.h.18.attn.c_proj.bias', 'transformer.h.18.ln_2.weight', 'transformer.h.18.ln_2.bias', 'transformer.h.18.mlp.c_fc.weight', 'transformer.h.18.mlp.c_fc.bias', 'transformer.h.18.mlp.c_proj.weight', 'transformer.h.18.mlp.c_proj.bias', 'transformer.h.19.ln_1.weight', 'transformer.h.19.ln_1.bias', 'transformer.h.19.attn.c_attn.weight', 'transformer.h.19.attn.c_attn.bias', 'transformer.h.19.attn.c_proj.weight', 'transformer.h.19.attn.c_proj.bias', 'transformer.h.19.ln_2.weight', 'transformer.h.19.ln_2.bias', 'transformer.h.19.mlp.c_fc.weight', 'transformer.h.19.mlp.c_fc.bias', 'transformer.h.19.mlp.c_proj.weight', 'transformer.h.19.mlp.c_proj.bias', 'transformer.h.20.ln_1.weight', 'transformer.h.20.ln_1.bias', 'transformer.h.20.attn.c_attn.weight', 'transformer.h.20.attn.c_attn.bias', 'transformer.h.20.attn.c_proj.weight', 'transformer.h.20.attn.c_proj.bias', 'transformer.h.20.ln_2.weight', 'transformer.h.20.ln_2.bias', 'transformer.h.20.mlp.c_fc.weight', 'transformer.h.20.mlp.c_fc.bias', 'transformer.h.20.mlp.c_proj.weight', 'transformer.h.20.mlp.c_proj.bias', 'transformer.h.21.ln_1.weight', 'transformer.h.21.ln_1.bias', 'transformer.h.21.attn.c_attn.weight', 'transformer.h.21.attn.c_attn.bias', 'transformer.h.21.attn.c_proj.weight', 'transformer.h.21.attn.c_proj.bias', 'transformer.h.21.ln_2.weight', 'transformer.h.21.ln_2.bias', 'transformer.h.21.mlp.c_fc.weight', 'transformer.h.21.mlp.c_fc.bias', 'transformer.h.21.mlp.c_proj.weight', 'transformer.h.21.mlp.c_proj.bias', 'transformer.h.22.ln_1.weight', 'transformer.h.22.ln_1.bias', 'transformer.h.22.attn.c_attn.weight', 'transformer.h.22.attn.c_attn.bias', 'transformer.h.22.attn.c_proj.weight', 'transformer.h.22.attn.c_proj.bias', 'transformer.h.22.ln_2.weight', 'transformer.h.22.ln_2.bias', 'transformer.h.22.mlp.c_fc.weight', 'transformer.h.22.mlp.c_fc.bias', 'transformer.h.22.mlp.c_proj.weight', 'transformer.h.22.mlp.c_proj.bias', 'transformer.ln_f.weight', 'transformer.ln_f.bias']\n",
      "Pruned model architecture: GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 1024)\n",
      "    (wpe): Embedding(1024, 1024)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-22): 23 x GPT2Block(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "pruned_model = make_pruned(base_model, [23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78271bd3-022b-4dd6-8a4f-f01ae75eb849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Federal law originates with the Constitution, which gives Congress the power to enact statutes for certain limited purposes like regulating interstate commerce. The United States Code is the official compilation and codification of the general and permanent federal statutes. Many statutes give executive branch agencies the power to create regulations, which are published in the Federal Register and codified into the Code of Federal Regulations. Regulations generally also carry the force of law under the Chevron doctrine. Many lawsuits turn on the meaning of a federal statute or regulation, and judicial interpretations of such meaning carry legal force under the principle of stare decisis.\n",
      "Who do the statutes give the power of creating regulations?\n",
      "The United States Code is the official compilation and codification of the general and permanent federal statutes. Many statutes give executive branch agencies the power to create regulations, which are published in the Federal Register and codified into the Code of Federal Regulations. Regulations\n",
      "\n",
      "\n",
      "Answer key:  executive branch agencies\n"
     ]
    }
   ],
   "source": [
    "# Test pruned model prediction\n",
    "answer_pruned = get_model_answer(INDEX, small_train_dataset, pruned_model, base_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "371bd741-5931-455b-9b2d-e689a8646605",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/hannahyl/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for setting up wandb:\n",
    "# import wandb\n",
    "# wandb.login()\n",
    "\n",
    "# wandb.init(\n",
    "#     project=\"gpt2-pruning\",\n",
    "#     config={\n",
    "#         \"batch_size\": BATCH_SIZE,\n",
    "#         \"learning_rate\": LEARNING_RATE,\n",
    "#         \"dataset\": \"SQuAD\",\n",
    "#     },\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8d9fee0-90f7-4dc8-bc2a-48dd6b43888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-5\n",
    "LOGGING_STEPS = 2\n",
    "SAVE_STEPS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12fd4ef1-8c15-4e2e-960a-9b2944e25277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_gpt2(model, \n",
    "                   tokenizer, \n",
    "                   train_dataset, \n",
    "                   valid_dataset, \n",
    "                   train_output_dir,\n",
    "                   save_model_dir):\n",
    "    # Create data collator for language modeling\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    # Set training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=train_output_dir, \n",
    "        evaluation_strategy = \"steps\", \n",
    "        logging_steps = LOGGING_STEPS,\n",
    "        logging_strategy = \"steps\",\n",
    "        save_steps = SAVE_STEPS,\n",
    "        num_train_epochs = EPOCHS,\n",
    "        per_device_train_batch_size = BATCH_SIZE,\n",
    "        per_device_eval_batch_size = BATCH_SIZE,\n",
    "        learning_rate = LEARNING_RATE,\n",
    "        # optim=\"paged_adamw_32bit\",\n",
    "        report_to=\"wandb\",\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    log_history = pd.DataFrame(trainer.state.log_history)\n",
    "    print(log_history)\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    model.save_pretrained(save_model_dir)\n",
    "    tokenizer.save_pretrained(save_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be302577-9af2-4bd8-a11b-25ac01960425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhylin\u001b[0m (\u001b[33muw-hannahyl\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/hannahyl/Github/599J/notebooks/wandb/run-20240219_224713-fcct9rna</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uw-hannahyl/huggingface/runs/fcct9rna' target=\"_blank\">prosperous-dumpling-1</a></strong> to <a href='https://wandb.ai/uw-hannahyl/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uw-hannahyl/huggingface' target=\"_blank\">https://wandb.ai/uw-hannahyl/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uw-hannahyl/huggingface/runs/fcct9rna' target=\"_blank\">https://wandb.ai/uw-hannahyl/huggingface/runs/fcct9rna</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 472.1118, 'train_samples_per_second': 0.212, 'train_steps_per_second': 0.015, 'train_loss': 6.195960453578404, 'epoch': 1.0}\n",
      "   train_runtime  train_samples_per_second  train_steps_per_second  \\\n",
      "0       472.1118                     0.212                   0.015   \n",
      "\n",
      "     total_flos  train_loss  epoch  step  \n",
      "0  1.857401e+14     6.19596    1.0     7  \n"
     ]
    }
   ],
   "source": [
    "fine_tune_gpt2(base_model, \n",
    "               base_tokenizer, \n",
    "               small_train_dataset['input_ids'], \n",
    "               small_valid_dataset['input_ids'],\n",
    "               \"train_log\",\n",
    "               \"trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1d45ceb-b2a6-4485-8577-f16dc471b434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Federal law originates with the Constitution, which gives Congress the power to enact statutes for certain limited purposes like regulating interstate commerce. The United States Code is the official compilation and codification of the general and permanent federal statutes. Many statutes give executive branch agencies the power to create regulations, which are published in the Federal Register and codified into the Code of Federal Regulations. Regulations generally also carry the force of law under the Chevron doctrine. Many lawsuits turn on the meaning of a federal statute or regulation, and judicial interpretations of such meaning carry legal force under the principle of stare decisis.\n",
      "Who do the statutes give the power of creating regulations?\n",
      "\n",
      "The statutes give the power to create regulations to the executive branch agencies, which are the federal agencies that are responsible for the administration of the federal government. The statutes also give the power to create regulations to the states, which are the states that\n",
      "\n",
      "\n",
      "Answer key:  executive branch agencies\n"
     ]
    }
   ],
   "source": [
    "answer_finetuned = get_model_answer(INDEX, small_train_dataset, base_model, base_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "059a64ae-c261-4ff2-ab0b-731a453d2977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.4325, 'learning_rate': 7.1428571428571436e-06, 'epoch': 0.29}\n",
      "{'eval_loss': 6.0187788009643555, 'eval_runtime': 35.4375, 'eval_samples_per_second': 0.705, 'eval_steps_per_second': 0.056, 'epoch': 0.29}\n",
      "{'loss': 6.1822, 'learning_rate': 4.2857142857142855e-06, 'epoch': 0.57}\n",
      "{'eval_loss': 5.372560024261475, 'eval_runtime': 34.8354, 'eval_samples_per_second': 0.718, 'eval_steps_per_second': 0.057, 'epoch': 0.57}\n",
      "{'loss': 5.8106, 'learning_rate': 1.4285714285714286e-06, 'epoch': 0.86}\n",
      "{'eval_loss': 5.044312953948975, 'eval_runtime': 34.615, 'eval_samples_per_second': 0.722, 'eval_steps_per_second': 0.058, 'epoch': 0.86}\n",
      "{'train_runtime': 467.221, 'train_samples_per_second': 0.214, 'train_steps_per_second': 0.015, 'train_loss': 6.378403731754848, 'epoch': 1.0}\n",
      "     loss  learning_rate  epoch  step  eval_loss  eval_runtime  \\\n",
      "0  7.4325       0.000007   0.29     2        NaN           NaN   \n",
      "1     NaN            NaN   0.29     2   6.018779       35.4375   \n",
      "2  6.1822       0.000004   0.57     4        NaN           NaN   \n",
      "3     NaN            NaN   0.57     4   5.372560       34.8354   \n",
      "4  5.8106       0.000001   0.86     6        NaN           NaN   \n",
      "5     NaN            NaN   0.86     6   5.044313       34.6150   \n",
      "6     NaN            NaN   1.00     7        NaN           NaN   \n",
      "\n",
      "   eval_samples_per_second  eval_steps_per_second  train_runtime  \\\n",
      "0                      NaN                    NaN            NaN   \n",
      "1                    0.705                  0.056            NaN   \n",
      "2                      NaN                    NaN            NaN   \n",
      "3                    0.718                  0.057            NaN   \n",
      "4                      NaN                    NaN            NaN   \n",
      "5                    0.722                  0.058            NaN   \n",
      "6                      NaN                    NaN        467.221   \n",
      "\n",
      "   train_samples_per_second  train_steps_per_second    total_flos  train_loss  \n",
      "0                       NaN                     NaN           NaN         NaN  \n",
      "1                       NaN                     NaN           NaN         NaN  \n",
      "2                       NaN                     NaN           NaN         NaN  \n",
      "3                       NaN                     NaN           NaN         NaN  \n",
      "4                       NaN                     NaN           NaN         NaN  \n",
      "5                       NaN                     NaN           NaN         NaN  \n",
      "6                     0.214                   0.015  1.780010e+14    6.378404  \n"
     ]
    }
   ],
   "source": [
    "fine_tune_gpt2(pruned_model, \n",
    "               base_tokenizer, \n",
    "               small_train_dataset['input_ids'], \n",
    "               small_valid_dataset['input_ids'],\n",
    "               \"train_log\",\n",
    "               \"trained_model_pruned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0b80c3f-9a27-40f7-81ce-b948bf3571aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Federal law originates with the Constitution, which gives Congress the power to enact statutes for certain limited purposes like regulating interstate commerce. The United States Code is the official compilation and codification of the general and permanent federal statutes. Many statutes give executive branch agencies the power to create regulations, which are published in the Federal Register and codified into the Code of Federal Regulations. Regulations generally also carry the force of law under the Chevron doctrine. Many lawsuits turn on the meaning of a federal statute or regulation, and judicial interpretations of such meaning carry legal force under the principle of stare decisis.\n",
      "Who do the statutes give the power of creating regulations?\n",
      "The United States Code is the official compilation and codification of the general and permanent federal statutes. Many statutes give executive branch agencies the power to create regulations, which are published in the Federal Register and codified into the Code of Federal Regulations. Regulations\n",
      "\n",
      "\n",
      "Answer key:  executive branch agencies\n"
     ]
    }
   ],
   "source": [
    "answer_finetuned_pruned = get_model_answer(INDEX, small_train_dataset, pruned_model, base_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4800ad8-c8ea-4257-ac6f-966a14cc1f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The United States Code is the official compilation and codification of the general and permanent federal statutes. Many statutes give executive branch agencies the power to create regulations, which are published in the Federal Register and codified into the Code of Federal Regulations. Regulations\n"
     ]
    }
   ],
   "source": [
    "print(\"The United States Code is the official compilation and codification of the general and permanent federal statutes. Many statutes give executive branch agencies the power to create regulations, which are published in the Federal Register and codified into the Code of Federal Regulations. Regulations\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
